{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 torchtext==0.18.0 torchdata==0.8.0 --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install xformers==0.0.28.post3\n",
    "\n",
    "!pip install accelerate==1.1.1 albumentations==1.4.21 audio-separator==0.24.1 black==23.12.1 diffusers==0.31.0 einops==0.8.0 ffmpeg-python==0.2.0 funasr==1.0.27 huggingface-hub==0.26.2 imageio==2.36.0\n",
    "!pip install imageio-ffmpeg==0.5.1 insightface==0.7.3 hydra-core==1.3.2 jax==0.4.35 mediapipe==0.10.18 modelscope==1.20.1 moviepy==1.0.3 numpy==1.26.4 omegaconf==2.3.0 onnxruntime-gpu==1.20.1\n",
    "!pip install opencv-python-headless==4.10.0.84 pillow==10.4.0 scikit-learn==1.5.2 scipy==1.14.1 transformers==4.46.3 tqdm==4.67.1 matplotlib matplotlib-inline\n",
    "\n",
    "!apt install aria2 ffmpeg -qqy\n",
    "\n",
    "%cd /workspace\n",
    "!git clone -b dev https://github.com/camenduru/memo\n",
    "%cd /workspace/memo\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/audio_proj/config.json -d /workspace/memo/checkpoints/audio_proj -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/audio_proj/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/audio_proj -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/diffusion_net/config.json -d /workspace/memo/checkpoints/diffusion_net -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/diffusion_net/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/diffusion_net -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/image_proj/config.json -d /workspace/memo/checkpoints/image_proj -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/image_proj/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/image_proj -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/audio_emotion_classifier/config.json -d /workspace/memo/checkpoints/misc/audio_emotion_classifier -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/audio_emotion_classifier/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/misc/audio_emotion_classifier -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/1k3d68.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o 1k3d68.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/2d106det.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o 2d106det.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/face_landmarker_v2_with_blendshapes.task -d /workspace/memo/checkpoints/misc/face_analysis/models -o face_landmarker_v2_with_blendshapes.task\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/genderage.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o genderage.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/glintr100.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o glintr100.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/face_analysis/models/scrfd_10g_bnkps.onnx -d /workspace/memo/checkpoints/misc/face_analysis/models -o scrfd_10g_bnkps.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/misc/vocal_separator/Kim_Vocal_2.onnx -d /workspace/memo/checkpoints/misc/vocal_separator -o Kim_Vocal_2.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/download_checks.json -d /workspace/memo/checkpoints/misc/vocal_separator -o download_checks.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/mdx_model_data.json -d /workspace/memo/checkpoints/misc/vocal_separator -o mdx_model_data.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/misc/vocal_separator/vr_model_data.json -d /workspace/memo/checkpoints/misc/vocal_separator -o vr_model_data.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/raw/main/reference_net/config.json -d /workspace/memo/checkpoints/reference_net -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/memoavatar/memo/resolve/main/reference_net/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/reference_net -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors -d /workspace/memo/checkpoints/vae -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/sd-vae-ft-mse/raw/main/config.json -d /workspace/memo/checkpoints/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/model.safetensors -d /workspace/memo/checkpoints/wav2vec2 -o model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/config.json -d /workspace/memo/checkpoints/wav2vec2 -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/preprocessor_config.json -d /workspace/memo/checkpoints/wav2vec2 -o preprocessor_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/resolve/main/model.pt -d /workspace/memo/checkpoints/emotion2vec_plus_large -o model.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/config.yaml -d /workspace/memo/checkpoints/emotion2vec_plus_large -o config.yaml\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/configuration.json -d /workspace/memo/checkpoints/emotion2vec_plus_large -o configuration.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/emotion2vec/emotion2vec_plus_large/raw/main/tokens.txt -d /workspace/memo/checkpoints/emotion2vec_plus_large -o tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/memo\n",
    "\n",
    "import os, torch\n",
    "from diffusers import AutoencoderKL, FlowMatchEulerDiscreteScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from memo.models.audio_proj import AudioProjModel\n",
    "from memo.models.image_proj import ImageProjModel\n",
    "from memo.models.unet_2d_condition import UNet2DConditionModel\n",
    "from memo.models.unet_3d import UNet3DConditionModel\n",
    "from memo.pipelines.video_pipeline import VideoPipeline\n",
    "from memo.utils.audio_utils import extract_audio_emotion_labels, preprocess_audio, resample_audio\n",
    "from memo.utils.vision_utils import preprocess_image, tensor_to_video\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "weight_dtype = torch.bfloat16\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"/workspace/memo/checkpoints/vae\").to(device=device, dtype=weight_dtype)\n",
    "reference_net = UNet2DConditionModel.from_pretrained(\"/workspace/memo/checkpoints\", subfolder=\"reference_net\", use_safetensors=True)\n",
    "diffusion_net = UNet3DConditionModel.from_pretrained(\"/workspace/memo/checkpoints\", subfolder=\"diffusion_net\", use_safetensors=True)\n",
    "image_proj = ImageProjModel.from_pretrained(\"/workspace/memo/checkpoints\", subfolder=\"image_proj\", use_safetensors=True)\n",
    "audio_proj = AudioProjModel.from_pretrained(\"/workspace/memo/checkpoints\", subfolder=\"audio_proj\", use_safetensors=True)\n",
    "\n",
    "vae.requires_grad_(False).eval()\n",
    "reference_net.requires_grad_(False).eval()\n",
    "diffusion_net.requires_grad_(False).eval()\n",
    "image_proj.requires_grad_(False).eval()\n",
    "audio_proj.requires_grad_(False).eval()\n",
    "\n",
    "reference_net.enable_xformers_memory_efficient_attention()\n",
    "diffusion_net.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "noise_scheduler = FlowMatchEulerDiscreteScheduler()\n",
    "pipeline = VideoPipeline(vae=vae, reference_net=reference_net, diffusion_net=diffusion_net, scheduler=noise_scheduler, image_proj=image_proj)\n",
    "pipeline.to(device=device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = \"assets/examples/dicaprio.jpg\"\n",
    "input_audio = \"assets/examples/speech.wav\"\n",
    "seed = 42\n",
    "resolution = 512\n",
    "num_generated_frames_per_clip = 16\n",
    "fps = 30\n",
    "num_init_past_frames = 2\n",
    "num_past_frames = 16\n",
    "inference_steps = 20\n",
    "cfg_scale = 3.5\n",
    "\n",
    "generator = torch.manual_seed(seed)\n",
    "img_size = (resolution, resolution)\n",
    "pixel_values, face_emb = preprocess_image(face_analysis_model=\"/workspace/memo/checkpoints/misc/face_analysis\", image_path=input_image, image_size=resolution)\n",
    "\n",
    "output_dir = \"/workspace/memo/outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cache_dir = os.path.join(output_dir, \"audio_preprocess\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "input_audio = resample_audio(input_audio, os.path.join(cache_dir, f\"{os.path.basename(input_audio).split('.')[0]}-16k.wav\"))\n",
    "\n",
    "audio_emb, audio_length = preprocess_audio(\n",
    "    wav_path=input_audio,\n",
    "    num_generated_frames_per_clip=num_generated_frames_per_clip,\n",
    "    fps=fps,\n",
    "    wav2vec_model=\"/workspace/memo/checkpoints/wav2vec2\",\n",
    "    vocal_separator_model=\"/workspace/memo/checkpoints/misc/vocal_separator/Kim_Vocal_2.onnx\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")\n",
    "audio_emotion, num_emotion_classes = extract_audio_emotion_labels(\n",
    "    model=\"/workspace/memo/checkpoints\",\n",
    "    wav_path=input_audio,\n",
    "    emotion2vec_model=\"/workspace/memo/checkpoints/emotion2vec_plus_large\",\n",
    "    audio_length=audio_length,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "video_frames = []\n",
    "num_clips = audio_emb.shape[0] // num_generated_frames_per_clip\n",
    "for t in tqdm(range(num_clips), desc=\"Generating video clips\"):\n",
    "    if len(video_frames) == 0:\n",
    "        past_frames = pixel_values.repeat(num_init_past_frames, 1, 1, 1)\n",
    "        past_frames = past_frames.to(dtype=pixel_values.dtype, device=pixel_values.device)\n",
    "        pixel_values_ref_img = torch.cat([pixel_values, past_frames], dim=0)\n",
    "    else:\n",
    "        past_frames = video_frames[-1][0]\n",
    "        past_frames = past_frames.permute(1, 0, 2, 3)\n",
    "        past_frames = past_frames[0 - num_past_frames :]\n",
    "        past_frames = past_frames * 2.0 - 1.0\n",
    "        past_frames = past_frames.to(dtype=pixel_values.dtype, device=pixel_values.device)\n",
    "        pixel_values_ref_img = torch.cat([pixel_values, past_frames], dim=0)\n",
    "\n",
    "    pixel_values_ref_img = pixel_values_ref_img.unsqueeze(0)\n",
    "    audio_tensor = (audio_emb[t * num_generated_frames_per_clip : min((t + 1) * num_generated_frames_per_clip, audio_emb.shape[0])].unsqueeze(0).to(device=audio_proj.device, dtype=audio_proj.dtype))\n",
    "    audio_tensor = audio_proj(audio_tensor)\n",
    "    audio_emotion_tensor = audio_emotion[t * num_generated_frames_per_clip : min((t + 1) * num_generated_frames_per_clip, audio_emb.shape[0])]\n",
    "\n",
    "    pipeline_output = pipeline(\n",
    "        ref_image=pixel_values_ref_img,\n",
    "        audio_tensor=audio_tensor,\n",
    "        audio_emotion=audio_emotion_tensor,\n",
    "        emotion_class_num=num_emotion_classes,\n",
    "        face_emb=face_emb,\n",
    "        width=img_size[0],\n",
    "        height=img_size[1],\n",
    "        video_length=num_generated_frames_per_clip,\n",
    "        num_inference_steps=inference_steps,\n",
    "        guidance_scale=cfg_scale,\n",
    "        generator=generator,\n",
    "    )\n",
    "    video_frames.append(pipeline_output.videos)\n",
    "\n",
    "video_frames = torch.cat(video_frames, dim=2)\n",
    "video_frames = video_frames.squeeze(0)\n",
    "video_frames = video_frames[:, :audio_length]\n",
    "\n",
    "tensor_to_video(video_frames, f\"/workspace/memo-{seed}-tost.mp4\", input_audio, fps=fps)\n",
    "\n",
    "result = f\"/workspace/memo-{seed}-tost.mp4\"\n",
    "\n",
    "from IPython.display import Video\n",
    "Video(result, embed=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
